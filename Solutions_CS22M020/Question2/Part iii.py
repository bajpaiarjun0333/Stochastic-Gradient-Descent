# -*- coding: utf-8 -*-
"""stochastic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qsPKVC9GPHDRD1AWxKup6oV6WIPtQElr
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb

from google.colab import files
uploaded=files.upload()

def loadData():
  df=pd.read_csv("Dataset.csv",header=None)
  df=df.to_numpy()
  x=df[:,0:100]
  y=df[:,100]
  n=x.shape[0]
  d=x.shape[1]
  return x,y,n,d

#Question 2 part i
#Analytical solution for wml
def computeWML(X,Y):
  w_ml=np.matmul(X.T,X)
  w_ml=np.linalg.inv(w_ml)
  w_ml=np.matmul(w_ml,X.T)
  w_ml=np.matmul(w_ml,Y)
  return w_ml

def initialize(d):
  w=np.ones(d)
  return w

def prediction(x,w):
  y_pred=np.matmul(x,w)
  return y_pred

def updateW(w,dw,alpha):
  wnew=w-alpha*dw
  return wnew

def computeGradient(w,x,y):
  #using the formula derived in the notes
  dw=np.matmul(x.T,x)
  dw=np.matmul(dw,w)
  dw=dw-np.matmul(x.T,y)
  dw=2*dw
  return dw

def generate_sample(X,Y,n,k):
  #d is used for representing dimensions
  indexes=np.random.randint(0,10000,k)
  x=[]
  y=[]
  for idx in indexes:
    x.append(X[idx,:])
    y.append(Y[idx])
  x=np.array(x)
  y=np.array(y)
  return (x,y)
#X,Y,n,d=loadData()
#x,y=generate_sample(X,Y,20,d)

def stochasticDescent(n_iter):
  X,Y,n,d=loadData()
  w=initialize(d)
  w_ml=computeWML(X,Y)
  diff=pd.DataFrame(columns=['Iteration','Cost'])
  result_idx=0
  for iter_num in range(n_iter):
    x,y=generate_sample(X,Y,n,100)
    err=np.linalg.norm(w-w_ml)
    w_old=w
    dw=computeGradient(w,x,y)
    w=updateW(w_old,dw,0.00003)
    if iter_num%10==0:
      diff.loc[result_idx]=[iter_num,err]
      result_idx=result_idx+1
  print("Final W after Gradient Descent")
  print(w)
  return w,diff

#plotting error in various iteration between w and w_ml
w,diff=stochasticDescent(8000)
diff[:]

def plot():
  plt.plot(diff['Iteration'],diff['Cost'])
  plt.xlabel("Number of iterations")
  plt.ylabel("L2 norm of ||w-wml||")
plot()

