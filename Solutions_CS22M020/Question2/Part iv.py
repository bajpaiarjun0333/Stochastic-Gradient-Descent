# -*- coding: utf-8 -*-
"""RidgeRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JGHsLOcIfRmcv4HnQLLffwgkfGVD5SqH
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb

from google.colab import files
uploaded=files.upload()

def loadData():
  df=pd.read_csv("Dataset.csv",header=None)
  df=df.to_numpy()
  x=df[:,0:100]
  y=df[:,100]
  n=x.shape[0]
  d=x.shape[1]
  return x,y,n,d
#unit testing code

#Question 2 part i
#Analytical solution for wml
def computeWML(x,y):
  w_ml=np.matmul(x.T,x)
  w_ml=np.linalg.inv(w_ml)
  w_ml=np.matmul(w_ml,x.T)
  w_ml=np.matmul(w_ml,y)
  return w_ml

x,y,n,d=loadData()
print(computeWML(x,y))

def initialize(d):
  w=np.ones(d)
  return w

def computeGradient(w,l):
  #Implementing ridge regression
  dw=np.matmul(x.T,x)
  dw=np.matmul(dw,w)
  dw=dw-np.matmul(x.T,y)
  dw=2*dw+2*l*w
  return dw

def prediction(x,w):
  y_pred=np.matmul(x,w)
  return y_pred

def updateW(w,dw,alpha):
  wnew=w-alpha*dw
  return wnew

def gradientDescent(n_iter,l,x,y,n,d):
  w=initialize(d)
  for iter_num in range(n_iter):
    w_old=w
    dw=computeGradient(w,l)
    w=updateW(w_old,dw,0.000003)
  return w

def computeError(y,x,w):
  y_pred=np.matmul(x,w)
  err=np.linalg.norm(y_pred-y)
  err=err*err
  return err

def loadDataTest():
  df=pd.read_csv("test_data.csv",header=None)
  df=df.to_numpy()
  x=df[:,0:100]
  y=df[:,100]
  n=x.shape[0]
  d=x.shape[1]
  return x,y,n,d
#unit testing code
x,y,n,d=loadData()
print(x.shape)
print(y.shape)
print(n)
print(d)

def generate_sample(X,Y,n,k):
  #d is used for representing dimensions
  indexes=np.random.randint(0,9999,k)
  x_train=[]
  y_train=[]
  for idx in indexes:
    x_train.append(X[idx,:])
    y_train.append(Y[idx])
  x_test=[]
  y_test=[]
  for idx in range(10000):
    if idx not in indexes:
      x_test.append(X[idx,:])
      y_test.append(Y[idx])
  x=np.array(x_train)
  y=np.array(y_test)
  x=np.array(x_test)
  y=np.array(y_test)
  return (x_train,y_train,x_test,y_test)

def kfold(x_train,y_train,x_test,y_test,n,d):
  # X,Y,n,d=loadData()
  # x_train=X[0:8000,:]
  # y_train=Y[0:8000]
  # x_test=X[8000:10000,:]
  # y_test=Y[8000:10000]
  w_list=[]
  for l in range(30):
    w=gradientDescent(3000,l,x_train,y_train,n,d)
    w_list.append(w)
    err=computeError(y_test,x_test,w)
    kdiff.loc[l]=[l,err]
  return w_list

from sklearn.model_selection import train_test_split

#plotting error in various iteration between w and w_ml
X,Y,n,d=loadData()
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=4)
kdiff=pd.DataFrame(columns=["Lambda","Average Error Crossvalidation"])
w_list=kfold(x_train,y_train,x_test,y_test,n,d)

def plot():
  plt.plot(kdiff['Lambda'],kdiff["Average Error Crossvalidation"])
  plt.xlabel("Lambda")
  plt.ylabel("Validation Error")
plot()

def loadDataTest():
  df=pd.read_csv("test_data.csv",header=None)
  df=df.to_numpy()
  x=df[:,0:100]
  y=df[:,100]
  n=x.shape[0]
  d=x.shape[1]
  return x,y,n,d
#unit testing code

#for value 17 it is minimum
wR=w_list[17]
#error on wR
x_test,y_test,n_test,d_test=loadDataTest()
errorR=computeError(y_test,x_test,wR)
x,y,n,d=loadData()
wMl=computeWML(x,y)
errorML=computeError(y_test,x_test,wMl)
print("Error due to ridge on test : ",errorR)
print("Error due to wML on test : ",errorML)

wR

